# ğŸš€ Athina Enhanced Vectorization Pipeline

A **secure multi-party computation (SMPC)** system for privacy-preserving data analysis with support for Boolean, Numeric, and Categorical data types.

## ğŸ¯ Quick Start

### 1. Start Services
```bash
# Start orchestrator
cd computations-orchestrator
docker-compose up -d

# Start vectorization service
cd ../vectorization-service
docker-compose up -d
```

### 2. Run Pipeline
```bash
cd trigger-vectorization-pipeline

# Development Mode (default - uses local datasets)
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --url metadata-test.json \
  --jobId my_job_001 \
  --clientsList client1 \
  --studyId study_12345

# Production Mode (requires PRODUCTION_MODE=true in docker-compose.yml)
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --studyId study_12345 \
  --jobId my_job_001 \
  --clientsList client1
```

### 3. Verify Results
```bash
# Check output files
docker exec -it vectorization_service_container ls -la /app/output/

# View processing logs
docker logs vectorization_service_container
```

## ğŸŒŸ Key Features

- âœ… **Feature Extraction Tool Integration**: Production mode API support for external datasets
- âœ… **Development/Production Modes**: Configurable via `PRODUCTION_MODE` environment variable  
- âœ… **Multi-Type Data Processing**: Boolean, Numeric, and Categorical features
- âœ… **Flexible Input**: Local files and URL-based datasets
- âœ… **Backward Compatible**: 100% compatible with existing systems
- âœ… **Secure**: SMPC-based privacy-preserving computation
- âœ… **Scalable**: Microservice architecture with Docker
- âœ… **Real-time**: HTTP API for immediate processing

## ğŸ“Š Supported Data Types

| Type | Input Fields | Output Vector | Length |
|------|--------------|---------------|--------|
| **Boolean** | `numOfNotNull`, `numOfTrue` | `[numOfNotNull, numOfTrue]` | 2 |
| **Numeric** | `numOfNotNull`, `min`, `max`, `avg`, `q1`, `q2`, `q3` | `[numOfNotNull, min, max, avg, q1, q2, q3]` | 7 |
| **Categorical** | `numOfNotNull`, `valueSet`, `cardinalityPerItem` | `[numOfNotNull, numUniqueValues, topValueCount]` | 3 |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Trigger       â”‚    â”‚ Vectorization   â”‚    â”‚ Computations    â”‚
â”‚   Script        â”‚â”€â”€â”€â–¶â”‚   Service       â”‚â”€â”€â”€â–¶â”‚ Orchestrator    â”‚
â”‚  (Python CLI)   â”‚    â”‚  (Flask API)    â”‚    â”‚  (Flask API)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   SMPC Client   â”‚    â”‚   Redis Store   â”‚
                     â”‚   (Privacy)     â”‚    â”‚   (Metadata)    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Pipeline Flow

1. **Trigger** â†’ User runs script with dataset location
2. **Fetch** â†’ Service fetches from URL or local file
3. **Analyze** â†’ Automatic data type detection
4. **Vectorize** â†’ Convert statistics to numerical vectors
5. **SMPC** â†’ Send to secure computation client
6. **Orchestrate** â†’ Notify orchestrator of completion
7. **Aggregate** â†’ Secure multi-party aggregation
8. **Results** â†’ Final aggregated results

## ğŸ“– Architecture Changes


## ğŸ› ï¸ Prerequisites

- Docker & Docker Compose
- Python 3.9+
- Git

## ğŸš€ Installation

```bash
# Start vectorization service
docker-compose up -d

# Verify installation
docker ps
# Should show: vectorization_service_container, smpc_client
```

## ğŸ’¡ Usage Examples

### Basic Usage - Local File
```bash
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --url my_dataset.json \
  --jobId job_001 \
  --clientsList client1 \
  --studyId study_001
```

### Multi-Client Scenario
```bash
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --url client_data.json \
  --jobId shared_job \
  --clientsList client1 client2 client3 \
  --studyId study_shared
```

### URL-based Dataset
```bash
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --url https://example.com/dataset.json \
  --jobId job_002 \
  --clientsList client1 \
  --studyId study_url
```

## ğŸ”§ Environment Configuration

### Production Mode Settings
```yaml
# In docker-compose.yml
environment:
  - PRODUCTION_MODE=false  # Set to 'true' to enable Feature Extraction Tool API
```

**Development Mode (default)**: `PRODUCTION_MODE=false`
- Uses local datasets (metadata-test.json, URL sources)
- Requires `--url` parameter

**Production Mode**: `PRODUCTION_MODE=true` 
- Calls Feature Extraction Tool API with `studyId`
- API endpoint: `https://localhost/dt4h/feast/api/Dataset?featureSetId={studyId}`
- `--url` parameter optional (uses API instead)

## ğŸ”Œ API Endpoints

- **Vectorization Service**: `http://localhost:5001/vectorize`
- **Orchestrator**: `http://localhost:5000/api/update`
- **SMPC Client**: `http://localhost:9001`
- **Redis**: `http://localhost:6379`

## ğŸ§ª Testing

### Run All Tests
```bash
cd vectorization-service
python run_tests.py
```

### Test Pipeline
```bash
# Test with new clientsList API and studyId support
cd ../trigger-vectorization-pipeline
python trigger_vectorization.py \
  --vectorizationServiceUrl http://localhost:5001 \
  --url metadata-test.json \
  --jobId test_job \
  --clientsList client1 \
  --studyId test_study_123
```

**Expected Results**: 19/19 tests pass âœ…

## ğŸ”§ Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Connection Error** | Use `host.docker.internal` for container-to-host communication |
| **File Not Found** | Ensure files exist in container: `docker exec -it <container> ls /app/` |
| **Port Conflicts** | Check ports 5000, 5001, 6379, 9001 are free |
| **Network Issues** | Verify all containers are running: `docker ps` |

### Debug Commands
```bash
# View logs
docker logs vectorization_service_container

# Access container
docker exec -it vectorization_service_container /bin/bash

# Check outputs
docker exec -it vectorization_service_container ls -la /app/output/
```

## ğŸ“Š Expected Output

### Successful Pipeline Run
```
INFO: Vectorization trigger request succeeded.
Response code: 200
Response body: {
  "message": "Vectorization completed.",
  "encodersCount": 1,
  "schemaCount": 6
}
```

### Generated Files
- `enhanced_dataset.json` - Original data with enhancements
- `encoders_only.json` - Encoded vectors for SMPC
- `schema.json` - Schema information for decoding

## ğŸŒ Service Ports

| Service | Port | Purpose |
|---------|------|---------|
| **Vectorization Service** | 5001 | Data processing API |
| **Orchestrator** | 5000 | Coordination & aggregation |
| **SMPC Client** | 9001 | Secure computation |
| **Redis** | 6379 | Metadata storage |

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/my-feature`
3. Commit changes: `git commit -am 'Add my feature'`
4. Push to branch: `git push origin feature/my-feature`
5. Submit Pull Request

## ğŸ“„ License

[License information]

---

## ğŸ‰ Ready to Get Started?

1. **ğŸš€ Start the service**: `docker-compose up -d`
2. **ğŸ§ª Run tests**: `python run_tests.py` (19/19 tests)
3. **ğŸ“‹ Check logs**: `Get-Content ../logs/vectorization-service.log`

**Happy vectorizing with secure multi-party computation!** ğŸš€ 